{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_Model = \"hkunlp/instructor-xl\"\n",
    "LLM_Model = \"google/flan-t5-large\"\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/content/langchain/qna_from_pdf_file_with_citation_using_local_Model/pdf_files/TRANSFORMERS.pdf\")          #path to PDF document\n",
    "documents = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=Embedding_Model)\n",
    "text = \"This is a test document.\"\n",
    "query_result = instructor_embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language generation pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_Model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_Model, torch_dtype=torch.float32)\n",
    "\n",
    "pipe = pipeline(\n",
    "   \"text2text-generation\",\n",
    "   model=model,\n",
    "   tokenizer=tokenizer,\n",
    "   max_length=512,\n",
    "   temperature=0,\n",
    "   top_p=0.95,\n",
    "   repetition_penalty=1.15\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "class CustomRetriever(BaseRetriever):\n",
    "   def _get_relevant_documents(\n",
    "       self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "   ) -> List[Document]:\n",
    "       return [Document(page_content=query)]\n",
    "retriever = CustomRetriever()\n",
    "retriever.get_relevant_documents(\"bar\")\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 return_source_documents=True)\n",
    "question = \"explain Power Transformers?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Ideal transformers can be characterized as?\"\n",
    "generated_text = qa(question)\n",
    "Generated_text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
